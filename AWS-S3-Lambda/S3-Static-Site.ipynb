{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating static web-sites using AWS S3 Object Storage\n",
    "\n",
    "You should first have\n",
    "- installed the awscli package to provide the aws command and also\n",
    "- configured either\n",
    "  - the ~/.aws/configure file with your AWS account credentials or\n",
    "  - created a sourceable ~/.aws/credentials.rc (can be in any location) file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "> cat ~/.aws/credentials.rc\n",
    "\n",
    "export AWS_ACCESS_KEY_ID=\"<your-access-key>\"\n",
    "export AWS_SECRET_ACCESS_KEY=\"<your-secret-access-key>\"\n",
    "export AWS_DEFAULT_REGION=us-west-1\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have chosen to use an rc file, source it as ```source <your-aws-credentials-rc-file>```, e.g."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    ". ~/.aws/credentials.rc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use the aws cli utility to access S3 commands.\n",
    "\n",
    "Let's investigate the available commands with ```aws s3 help```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S3()                                                                      S3()\n",
      "\n",
      "\n",
      "\n",
      "NAME\n",
      "       s3 -\n",
      "\n",
      "DESCRIPTION\n",
      "       This  section  explains  prominent concepts and notations in the set of\n",
      "       high-level S3 commands provided.\n",
      "\n",
      "   Path Argument Type\n",
      "       Whenever using a command, at least one path argument must be specified.\n",
      "       There are two types of path arguments: LocalPath and S3Uri.\n",
      "\n",
      "       LocalPath: represents the path of a local file or directory.  It can be\n",
      "       written as an absolute path or relative path.\n",
      "\n",
      "       S3Uri: represents the location of a S3 object, prefix, or bucket.  This\n",
      "       must  be  written in the form s3://mybucket/mykey where mybucket is the\n",
      "       specified S3 bucket, mykey is the specified S3 key.  The path  argument\n",
      "       must  begin with s3:// in order to denote that the path argument refers\n",
      "       to a S3 object. Note that prefixes are separated  by  forward  slashes.\n",
      "       For  example, if the S3 object myobject had the prefix myprefix, the S3\n",
      "       key would be myprefix/myobject, and if the object  was  in  the  bucket\n",
      "       mybucket, the S3Uri would be s3://mybucket/myprefix/myobject.\n",
      "\n",
      "   Order of Path Arguments\n",
      "       Every  command  takes  one or two positional path arguments.  The first\n",
      "       path argument represents the source, which is the local  file/directory\n",
      "       or  S3  object/prefix/bucket  that  is being referenced.  If there is a\n",
      "       second path argument, it represents the destination, which is the local\n",
      "       file/directory  or  S3  object/prefix/bucket that is being operated on.\n",
      "       Commands with only one path argument do not have a destination  because\n",
      "       the operation is being performed only on the source.\n",
      "\n",
      "   Single Local File and S3 Object Operations\n",
      "       Some  commands  perform operations only on single files and S3 objects.\n",
      "       The following commands are single file/object operations if no --recur-\n",
      "       sive flag is provided.\n",
      "\n",
      "          o cp\n",
      "\n",
      "          o mv\n",
      "\n",
      "          o rm\n",
      "\n",
      "       For  this  type of operation, the first path argument, the source, must\n",
      "       exist and be a local file or S3 object.  The second path argument,  the\n",
      "       destination,  can  be  the  name  of  a local file, local directory, S3\n",
      "       object, S3 prefix, or S3 bucket.\n",
      "\n",
      "       The destination is indicated as a local directory,  S3  prefix,  or  S3\n",
      "       bucket if it ends with a forward slash or back slash.  The use of slash\n",
      "       depends on the path argument type.  If the path argument  is  a  Local-\n",
      "       Path,  the type of slash is the separator used by the operating system.\n",
      "       If the path is a S3Uri, the forward slash must always be  used.   If  a\n",
      "       slash  is at the end of the destination, the destination file or object\n",
      "       will adopt the name of the source file or object.  Otherwise, if  there\n",
      "       is no slash at the end, the file or object will be saved under the name\n",
      "       provided.  See examples in cp and mv to illustrate this description.\n",
      "\n",
      "   Directory and S3 Prefix Operations\n",
      "       Some commands only perform operations on the contents of a local direc-\n",
      "       tory  or  S3 prefix/bucket.  Adding or omitting a forward slash or back\n",
      "       slash to the end of any path argument, depending on its type, does  not\n",
      "       affect  the  results  of  the  operation.   The following commands will\n",
      "       always result in a directory or S3 prefix/bucket operation:\n",
      "\n",
      "       o sync\n",
      "\n",
      "       o mb\n",
      "\n",
      "       o rb\n",
      "\n",
      "       o ls\n",
      "\n",
      "   Use of Exclude and Include Filters\n",
      "       Currently, there is no support for the use of UNIX style wildcards in a\n",
      "       command's  path  arguments.   However,  most  commands  have  --exclude\n",
      "       \"<value>\" and --include  \"<value>\"  parameters  that  can  achieve  the\n",
      "       desired  result.   These  parameters perform pattern matching to either\n",
      "       exclude or include a particular file or object.  The following  pattern\n",
      "       symbols are supported.\n",
      "\n",
      "          o *: Matches everything\n",
      "\n",
      "          o ?: Matches any single character\n",
      "\n",
      "          o [sequence]: Matches any character in sequence\n",
      "\n",
      "          o [!sequence]: Matches any character not in sequence\n",
      "\n",
      "       Any  number of these parameters can be passed to a command.  You can do\n",
      "       this by providing an --exclude or --include  argument  multiple  times,\n",
      "       e.g.   --include  \"*.txt\"  --include  \"*.png\".  When there are multiple\n",
      "       filters, the rule is the filters that appear later in the command  take\n",
      "       precedence  over filters that appear earlier in the command.  For exam-\n",
      "       ple, if the filter parameters passed to the command were\n",
      "\n",
      "          --exclude \"*\" --include \"*.txt\"\n",
      "\n",
      "       All files will be excluded from the command  except  for  files  ending\n",
      "       with  .txt   However, if the order of the filter parameters was changed\n",
      "       to\n",
      "\n",
      "          --include \"*.txt\" --exclude \"*\"\n",
      "\n",
      "       All files will be excluded from the command.\n",
      "\n",
      "       Each filter is evaluated against the source directory.  If  the  source\n",
      "       location is a file instead of a directory, the directory containing the\n",
      "       file is used as the source directory.  For example, suppose you had the\n",
      "       following directory structure:\n",
      "\n",
      "          /tmp/foo/\n",
      "            .git/\n",
      "            |---config\n",
      "            |---description\n",
      "            foo.txt\n",
      "            bar.txt\n",
      "            baz.jpg\n",
      "\n",
      "       In  the  command aws s3 sync /tmp/foo s3://bucket/ the source directory\n",
      "       is /tmp/foo.  Any include/exclude filters will be  evaluated  with  the\n",
      "       source  directory prepended.  Below are several examples to demonstrate\n",
      "       this.\n",
      "\n",
      "       Given the directory structure above and the command aws s3 cp  /tmp/foo\n",
      "       s3://bucket/  --recursive --exclude \".git/*\", the files .git/config and\n",
      "       .git/description will be excluded from the files to upload because  the\n",
      "       exclude  filter  .git/*  will  have the source prepended to the filter.\n",
      "       This means that:\n",
      "\n",
      "          /tmp/foo/.git/* -> /tmp/foo/.git/config       (matches, should exclude)\n",
      "          /tmp/foo/.git/* -> /tmp/foo/.git/description  (matches, should exclude)\n",
      "          /tmp/foo/.git/* -> /tmp/foo/foo.txt  (does not match, should include)\n",
      "          /tmp/foo/.git/* -> /tmp/foo/bar.txt  (does not match, should include)\n",
      "          /tmp/foo/.git/* -> /tmp/foo/baz.jpg  (does not match, should include)\n",
      "\n",
      "       The command aws s3  cp  /tmp/foo/  s3://bucket/  --recursive  --exclude\n",
      "       \"ba*\" will exclude /tmp/foo/bar.txt and /tmp/foo/baz.jpg:\n",
      "\n",
      "          /tmp/foo/ba* -> /tmp/foo/.git/config      (does not match, should include)\n",
      "          /tmp/foo/ba* -> /tmp/foo/.git/description (does not match, should include)\n",
      "          /tmp/foo/ba* -> /tmp/foo/foo.txt          (does not match, should include)\n",
      "          /tmp/foo/ba* -> /tmp/foo/bar.txt  (matches, should exclude)\n",
      "          /tmp/foo/ba* -> /tmp/foo/baz.jpg  (matches, should exclude)\n",
      "\n",
      "       Note that, by default, all files are included.  This means that provid-\n",
      "       ing only an --include filter will not  change  what  files  are  trans-\n",
      "       ferred.   --include  will only re-include files that have been excluded\n",
      "       from an --exclude filter.  If you only want to upload files with a par-\n",
      "       ticular extension, you need to first exclude all files, then re-include\n",
      "       the files with the particular extension.  This command will upload only\n",
      "       files ending with .jpg:\n",
      "\n",
      "          aws s3 cp /tmp/foo/ s3://bucket/ --recursive --exclude \"*\" --include \"*.jpg\"\n",
      "\n",
      "       If  you wanted to include both .jpg files as well as .txt files you can\n",
      "       run:\n",
      "\n",
      "          aws s3 cp /tmp/foo/ s3://bucket/ --recursive \\\n",
      "              --exclude \"*\" --include \"*.jpg\" --include \"*.txt\"\n",
      "\n",
      "       See 'aws help' for descriptions of global parameters.\n",
      "\n",
      "SYNOPSIS\n",
      "          aws s3 <Command> [<Arg> ...]\n",
      "\n",
      "OPTIONS\n",
      "       None\n",
      "\n",
      "       See 'aws help' for descriptions of global parameters.\n",
      "\n",
      "AVAILABLE COMMANDS\n",
      "       o cp\n",
      "\n",
      "       o ls\n",
      "\n",
      "       o mb\n",
      "\n",
      "       o mv\n",
      "\n",
      "       o presign\n",
      "\n",
      "       o rb\n",
      "\n",
      "       o rm\n",
      "\n",
      "       o sync\n",
      "\n",
      "       o website\n",
      "\n",
      "\n",
      "\n",
      "                                                                          S3()\n"
     ]
    }
   ],
   "source": [
    "aws s3 help "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "S3 is organized into buckets, each of which contains a hierarchy of named objects.\n",
    "\n",
    "The bucket name itself must be globally **unique** - across all AWS accounts not just yours.\n",
    "\n",
    "Let's see if we have any buckets of our own using the ```aws s3 ls``` command.\n",
    "You won't have any buckets if you just created your account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "aws s3 ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create our own bucket, we use the 'mb' or *make bucket* command to do this as\n",
    "    ```aws s3 mb s3://<my-bucket-name>```\n",
    "    \n",
    "Note that we always address buckets in URL form ```s3://<bucket>```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "make_bucket failed: s3://mybucket An error occurred (BucketAlreadyExists) when calling the CreateBucket operation: The requested bucket name is not available. The bucket namespace is shared by all users of the system. Please select a different name and try again.\n"
     ]
    },
    {
     "ename": "",
     "evalue": "1",
     "output_type": "error",
     "traceback": []
    }
   ],
   "source": [
    "aws s3 mb s3://mybucket"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course someone got there first with our \"mybucket\" name !\n",
    "\n",
    "So create a unique name, e.g. for myself:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "make_bucket: mjbright-static-site\n"
     ]
    }
   ],
   "source": [
    "aws s3 mb s3://mjbright-static-site"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-24 06:41:23 mjbright-static-site\n"
     ]
    }
   ],
   "source": [
    "aws s3 ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That was lucky !\n",
    "\n",
    "Now let's add some files to our bucket.\n",
    "\n",
    "We can do this with the cp or sync commands.\n",
    "\n",
    "Let's create an HTML index file and copy this into our bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-rw-r-- 1 user1 user1 74 Jan 24 06:41 website/index.html\n"
     ]
    }
   ],
   "source": [
    "mkdir -p website;\n",
    "\n",
    "cat > website/index.html <<EOF\n",
    "<html>\n",
    "<body>\n",
    "    <h1> My first amazing web site !! </h1>\n",
    "</body>\n",
    "</html>\n",
    "EOF\n",
    "\n",
    "ls -al website/index.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed 74 Bytes/74 Bytes (4.2 KiB/s) with 1 file(s) remaining\r",
      "upload: website/index.html to s3://mjbright-static-site/index.html\n"
     ]
    }
   ],
   "source": [
    "aws s3 sync ./website s3://mjbright-static-site"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-24 06:41:41         74 index.html\n"
     ]
    }
   ],
   "source": [
    "aws s3 ls s3://mjbright-static-site"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So it seems we have created a static web site.\n",
    "\n",
    "We can use the handy S3 command website to declare that this is a website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "aws s3 website s3://mjbright-static-site --index-document index.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The site should be available at http://<bucketname>.s3-website-<region>.amazonaws.com, which in this case would be:\n",
    "    \n",
    "    http://mjbright-static-site.s3-website-us-west-1.amazonaws.com/\n",
    "    \n",
    "However, if we visit that web page we will get an error telling us that we cannot access the page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2019-01-24 06:41:59--  http://mjbright-static-site.s3-website-us-west-1.amazonaws.com/index.html\n",
      "Resolving mjbright-static-site.s3-website-us-west-1.amazonaws.com (mjbright-static-site.s3-website-us-west-1.amazonaws.com)... 52.219.20.66\n",
      "Connecting to mjbright-static-site.s3-website-us-west-1.amazonaws.com (mjbright-static-site.s3-website-us-west-1.amazonaws.com)|52.219.20.66|:80... connected.\n",
      "HTTP request sent, awaiting response... 403 Forbidden\n",
      "2019-01-24 06:41:59 ERROR 403: Forbidden.\n",
      "\n"
     ]
    },
    {
     "ename": "",
     "evalue": "8",
     "output_type": "error",
     "traceback": []
    }
   ],
   "source": [
    "wget -O - http://mjbright-static-site.s3-website-us-west-1.amazonaws.com/index.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In fact we first need to enable website hosting from the bucket **and** enable public access to the index.html file.\n",
    "\n",
    "We can do this via the AWS Console.\n",
    "\n",
    "Connect to the console with your credentials and then navigate to https://s3.console.aws.amazon.com/s3/buckets/.\n",
    "\n",
    "\n",
    "![](images/BucketProperties.JPG)\n",
    "\n",
    "You should see your bucket listed here.\n",
    "\n",
    "Click on the line (not on the bucket name which is a link, but under the Access or Region column) to see the following dropdown menu\n",
    "\n",
    "![](images/BucketProperties-BeforeWebHostingEnabled.JPG)\n",
    "\n",
    "Click on \"Enable Web hosting\" and you should see:\n",
    "\n",
    "<!-- ![](images/BucketProperties-Settings-EnableWebsiteHosting.JPG) -->\n",
    "\n",
    "![](images/BucketProperties-Settings-Enabled_WebsiteHosting.JPG)\n",
    "\n",
    "But we still cannot access our site as we need to enable public access to the index.html file.\n",
    "\n",
    "Click on the bucket name to be taken to a list of files in the bucket, select the index.html file and then \"*Make Public*\" in the dropdown \"*Actions*\" menu:\n",
    "\n",
    "![](images/Make_index_public.JPG)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You now should be able to access your site using a browser, or from the command-line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wget -q -O - http://mjbright-static-site.s3-website-us-west-1.amazonaws.com/index.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now create something a little more like a website.\n",
    "\n",
    "We'll use the Pelican command (installed as a Python module).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pelican -o PELICAN -q\n",
    "\n",
    "cp -a PELICAN/* website/\n",
    "\n",
    "aws s3 sync website/ s3://mjbright-static-site/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wget -q -O - http://mjbright-static-site.s3-website-us-west-1.amazonaws.com/index.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately this broke our website as we have not yet set permissions for the new files we just added to our website.\n",
    "\n",
    "Go back to the bucket list at  https://s3.console.aws.amazon.com/s3/buckets/, drill down into your website bucket.\n",
    "\n",
    "Then select all items within the bucket and from the Access drop-down menu select \"*Make Public*\"\n",
    "\n",
    "Your website should now be accessible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wget -q -O - http://mjbright-static-site.s3-website-us-west-1.amazonaws.com/index.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More Reading\n",
    "\n",
    "You can find more details about S3 website hosting here: https://docs.aws.amazon.com/AmazonS3/latest/dev/WebsiteHosting.html\n",
    "\n",
    "An article describing the use of S3 for static website hosting including use of https, DNS routing\n",
    "https://medium.freecodecamp.org/how-to-host-a-static-website-with-s3-cloudfront-and-route53-7cbb11d4aeea\n",
    "\n",
    "# Cleanup\n",
    "\n",
    "Note that we can use the ```aws s3 rm``` command to remove files from the bucket and ```aws s3 rb``` command to remove a bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aws s3 rm s3://mjbright-static-site --recursive\n",
    "aws s3 rb s3://mjbright-static-site"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's also possible to remoce the bucket directly using the ```--force``` option:\n",
    "    ```aws s3 rb --force s3://mjbright-static-site```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Bash",
   "language": "bash",
   "name": "bash"
  },
  "language_info": {
   "codemirror_mode": "shell",
   "file_extension": ".sh",
   "mimetype": "text/x-sh",
   "name": "bash"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
